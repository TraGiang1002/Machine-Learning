from __future__ import division, print_function, unicode_literals
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist
# generate list of data points
np.random.seed(22)
means = [[2, 2], [4, 2]]
cov = [[.7, 0], [0, .7]]
N = 20
X1 = np.random.multivariate_normal(means[0], cov, N)
X2 = np.random.multivariate_normal(means[1], cov, N)
plt.plot(X1[:, 0], X1[:, 1], 'bs', markersize = 8, alpha = 1)
plt.plot(X2[:, 0], X2[:, 1], 'ro', markersize = 8, alpha = 1)
plt.axis('equal')
plt.ylim(0, 4)
plt.xlim(0, 5)
# hide tikcs
cur_axes = plt.gca()
cur_axes.axes.get_xaxis().set_ticks([])
cur_axes.axes.get_yaxis().set_ticks([])
plt.xlabel('$x_1$', fontsize = 20)
plt.ylabel('$x_2$', fontsize = 20)
# save the figure to an image first
plt.savefig('logistic_2d.png', bbox_inches='tight', dpi = 300)
plt.show()
def sigmoid(s):
    return 1/(1 + np.exp(-s)) # calculate sigmoid function
def logistic_sigmoid_regression(X, y, w_init, eta, tol = 1e-4, max_count = 10000):
     w = [w_init]
     it = 0
     N = X.shape[1]
     d = X.shape[0]
     count = 0
     check_w_after = 20
     while count < max_count:
         # mix data for stochastic gradient descent method
         mix_id = np.random.permutation(N)
         for i in mix_id:
             xi = X[:, i].reshape(d, 1)
             yi = y[i]
             zi = sigmoid(np.dot(w[-1].T, xi))
             w_new = w[-1] + eta*(yi - zi)*xi
             count += 1
             # stopping criteria
             if count%check_w_after == 0:
                 if np.linalg.norm(w_new - w[-check_w_after]) < tol:
                    return w
             w.append(w_new)
     return w
X = np.concatenate((X1, X2), axis = 0).T
y = np.concatenate((np.zeros((1, N)), np.ones((1, N))), axis = 1).T
# Xbar
X = np.concatenate((np.ones((1, 2*N)), X), axis = 0)
eta = 0.05
d = X.shape[0]
w_init = np.random.randn(d, 1) # initialize parameters w = w_init
# call logistic_sigmoid_regression procedure
w = logistic_sigmoid_regression(X, y, w_init, eta, tol = 1e-4, max_count= 10000)
# print out the parameter
print(w[-1])
# Make data.
x1m = np.arange(-1, 6, 0.025) # generate data coord. X1
xlen = len(x1m)
x2m = np.arange(0, 4, 0.025) # generate data coord. X2
x2en = len(x2m)
x1m, x2m = np.meshgrid(x1m, x2m) # create mesh grid X = (X1, X2)
# now assign the parameter w0, w1, w2 from array w which was computed above
w0 = w[-1][0][0]
w1 = w[-1][1][0]
w2 = w[-1][2][0]
# calculate probability zm=P(c|x)=sigmoid(w^Tx)=sigmoid(w0+w1*x1m+w2*x2m)
zm = sigmoid(w0 + w1*x1m + w2*x2m)
# plot contour of prob. zm by the saturation of blue and red
# more red <=> prob. that data point belong to red class is higher & vise versa
CS = plt.contourf(x1m, x2m, zm, 200, cmap='jet')
# finally, plot the data and take a look
plt.plot(X1[:, 0], X1[:, 1], 'bs', markersize = 8, alpha = 1)
plt.plot(X2[:, 0], X2[:, 1], 'ro', markersize = 8, alpha = 1)
plt.axis('equal')
plt.ylim(0, 4)
plt.xlim(0, 5)
# hide tikcs
cur_axes = plt.gca()
cur_axes.axes.get_xaxis().set_ticks([])
cur_axes.axes.get_yaxis().set_ticks([])
plt.xlabel('$x_1$', fontsize = 20)
plt.ylabel('$x_2$', fontsize = 20)
plt.savefig('logistic_2d_2.png', bbox_inches='tight', dpi = 300)
plt.show()

import numpy as np
from sklearn import linear_model
from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score

# generate list of data points
np.random.seed(22)
means = [[2, 2], [4, 2]]

cov = [[.7, 0], [0, .7]]
N = 20
X1 = np.random.multivariate_normal(means[0], cov, N)
X2 = np.random.multivariate_normal(means[1], cov, N)

X = np.concatenate((X1, X2), axis=0).T
y = np.concatenate((np.zeros((1, N)), np.ones((1, N))), axis=1).T
# Xbar
X = np.concatenate((np.ones((1, 2 * N)), X), axis=0).T
# Sử dụng thư viện scikit-learn
# X = X.T
logReg = linear_model.LogisticRegression(penalty=None, fit_intercept=0)
# Training & compute weights w
logReg.fit(X, y)
y_pred = logReg.predict(X)
print(X)
print(y.T)
print(logReg.coef_)
print(accuracy_score(y,y_pred))
# recall_score, precision_score, f1_score
print(recall_score(y,y_pred))
print(precision_score(y,y_pred))
print(f1_score(y,y_pred))
print(confusion_matrix(y, y_pred))